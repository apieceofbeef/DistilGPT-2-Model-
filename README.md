# DistilGPT-2-Model-
This repository contains a DistilGPT-2 model fine-tuned on 100â€¯MB of text data for simple text generation and chat. It supports CPU and GPU inference and lightweight training on small datasets.
